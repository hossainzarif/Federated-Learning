{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"NomukEAzPMMe"}},{"cell_type":"code","source":"import torchvision\nimport torchvision.transforms as transforms\nimport torch\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader, random_split, Subset, WeightedRandomSampler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport random\nimport math\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset\nfrom sklearn.metrics import accuracy_score\nrandom.seed(7)","metadata":{"id":"YUIYSeDiPMMi","execution":{"iopub.status.busy":"2022-09-09T08:13:18.025170Z","iopub.execute_input":"2022-09-09T08:13:18.025719Z","iopub.status.idle":"2022-09-09T08:13:20.697796Z","shell.execute_reply.started":"2022-09-09T08:13:18.025606Z","shell.execute_reply":"2022-09-09T08:13:20.696781Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{"id":"MmEiE2vNPMMk"}},{"cell_type":"code","source":"# # train and test set\n# train_set=torchvision.datasets.FashionMNIST(root= './MNISTT_data/',\n#                                   download= True,\n#                                   transform= transforms.Compose([transforms.ToTensor()]))\n# test_set=torchvision.datasets.FashionMNIST(root= './MNISTT_data/',\n#                                   train=False,\n#                                   download= True,\n#                                   transform= transforms.Compose([transforms.ToTensor()]))\n","metadata":{"id":"vTu0P0xiPMMk","outputId":"2b96cc09-f55a-42d0-eaaf-97c9e476268d","execution":{"iopub.status.busy":"2022-09-09T08:13:20.699841Z","iopub.execute_input":"2022-09-09T08:13:20.700838Z","iopub.status.idle":"2022-09-09T08:13:20.705867Z","shell.execute_reply.started":"2022-09-09T08:13:20.700782Z","shell.execute_reply":"2022-09-09T08:13:20.704464Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output \n\n\n! pip install gdown\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:20.707522Z","iopub.execute_input":"2022-09-09T08:13:20.707962Z","iopub.status.idle":"2022-09-09T08:13:45.132726Z","shell.execute_reply.started":"2022-09-09T08:13:20.707922Z","shell.execute_reply":"2022-09-09T08:13:45.131373Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import gdown\nurl = 'https://drive.google.com/uc?id=1MAakvkGyUNiqZNKnnpsreVGDCJnS5Czx' \noutput_X = 'X_UCM.pickle'\ngdown.download(url, output_X, quiet=False)\n\nurl_y = 'https://drive.google.com/uc?id=1mSLimMpZXqYuUJgL4D4bhtnlwzhKTgOh' \noutput_Y = 'Y_UCM.pickle'\ngdown.download(url_y, output_Y, quiet=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:45.135784Z","iopub.execute_input":"2022-09-09T08:13:45.136492Z","iopub.status.idle":"2022-09-09T08:13:51.239403Z","shell.execute_reply.started":"2022-09-09T08:13:45.136451Z","shell.execute_reply":"2022-09-09T08:13:51.238432Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pickle\n\npickle_in = open(\"./X_UCM.pickle\",\"rb\")\nX = pickle.load(pickle_in)\n\npickle_in = open(\"./Y_UCM.pickle\",\"rb\")\nY = pickle.load(pickle_in)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:51.241006Z","iopub.execute_input":"2022-09-09T08:13:51.241652Z","iopub.status.idle":"2022-09-09T08:13:51.844434Z","shell.execute_reply.started":"2022-09-09T08:13:51.241610Z","shell.execute_reply":"2022-09-09T08:13:51.843472Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom matplotlib import pyplot as plt\nimport random\n\nfig = plt.figure(figsize=(10, 7))\n\nrows = 2\ncolumns = 2\n\n\nfor i in range (0,4):\n    fig.add_subplot(rows, columns, i+1)\n    rand = random.randint(0, 2000)\n    plt.imshow(X[rand])\n    plt.axis('off')\n    plt.title(Y[rand])","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:51.845876Z","iopub.execute_input":"2022-09-09T08:13:51.846367Z","iopub.status.idle":"2022-09-09T08:13:52.394274Z","shell.execute_reply.started":"2022-09-09T08:13:51.846329Z","shell.execute_reply":"2022-09-09T08:13:52.393441Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(\nX, Y, test_size=0.1, random_state=42,stratify=Y)\n\nx_test, x_valid, y_test, y_valid = train_test_split(\nx_test, y_test, test_size=0.2, random_state=42,stratify=y_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:52.395294Z","iopub.execute_input":"2022-09-09T08:13:52.395723Z","iopub.status.idle":"2022-09-09T08:13:52.546339Z","shell.execute_reply.started":"2022-09-09T08:13:52.395678Z","shell.execute_reply":"2022-09-09T08:13:52.545273Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:52.547718Z","iopub.execute_input":"2022-09-09T08:13:52.548068Z","iopub.status.idle":"2022-09-09T08:13:52.557653Z","shell.execute_reply.started":"2022-09-09T08:13:52.548033Z","shell.execute_reply":"2022-09-09T08:13:52.556689Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:52.559452Z","iopub.execute_input":"2022-09-09T08:13:52.560110Z","iopub.status.idle":"2022-09-09T08:13:52.567478Z","shell.execute_reply.started":"2022-09-09T08:13:52.560069Z","shell.execute_reply":"2022-09-09T08:13:52.566544Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"x_train=torch.tensor(x_train)\nx_train.shape\nx_train=torch.reshape(x_train, (1890,3,256, 256))\nx_train.shape\ny_train = torch.tensor(y_train)\ny_test = torch.tensor(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:52.572343Z","iopub.execute_input":"2022-09-09T08:13:52.573182Z","iopub.status.idle":"2022-09-09T08:13:52.852117Z","shell.execute_reply.started":"2022-09-09T08:13:52.573155Z","shell.execute_reply":"2022-09-09T08:13:52.850972Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"x_test=torch.tensor(x_test)\nx_test.shape\nx_test=torch.reshape(x_test, (168,3,256, 256))\nx_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:52.853747Z","iopub.execute_input":"2022-09-09T08:13:52.854143Z","iopub.status.idle":"2022-09-09T08:13:52.887802Z","shell.execute_reply.started":"2022-09-09T08:13:52.854110Z","shell.execute_reply":"2022-09-09T08:13:52.886719Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:52.889746Z","iopub.execute_input":"2022-09-09T08:13:52.890123Z","iopub.status.idle":"2022-09-09T08:13:52.896464Z","shell.execute_reply.started":"2022-09-09T08:13:52.890087Z","shell.execute_reply":"2022-09-09T08:13:52.895404Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Analyze distributions","metadata":{"id":"4RATWnq-PMMm"}},{"cell_type":"code","source":" output_mapping = {\n                 0: \"agricultural\",\n                 1:\"airplane\",\n                 2: \"baseballdiamond\",\n                 3: \"beach\",\n                 4: \"buildings\",\n                 5: \"chaparral\", \n                 6: \"denseresidential\", \n                 7: \"forest\",\n                 8: \"freeway\",\n                 9: \"golfcourse\",\n                 10:\"harbor\",\n                 11:\"intersection\",\n                 12:\"mediumresidential\",\n                 13:\"mobilehomepark\",\n                 14:\"overpass\",\n                 15:\"parkinglot\",\n                 16:\"river\",\n                 17:\"runway\",\n                 18:\"sparseresidential\",\n                 19:\"storagetanks\",\n                 20:\"tenniscourt\"\n                 }\n    \n ","metadata":{"id":"IbwQF23YPMMm","execution":{"iopub.status.busy":"2022-09-09T08:13:52.898187Z","iopub.execute_input":"2022-09-09T08:13:52.898907Z","iopub.status.idle":"2022-09-09T08:13:52.907145Z","shell.execute_reply.started":"2022-09-09T08:13:52.898871Z","shell.execute_reply":"2022-09-09T08:13:52.905966Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_counts=y_train.unique(return_counts=True)\ntest_counts=y_test.unique(return_counts=True)\n\nfor i in range(21):\n    t=(100*train_counts[1][i]/(train_counts[1][i] + test_counts[1][i])).item()\n    v=(100*test_counts[1][i]/(train_counts[1][i] + test_counts[1][i])).item()\n    print( (output_mapping[i] + \": Train= {t:.2f}%;  Test= {v:.2f}%\").format(t=t, v=v))\n    ","metadata":{"id":"ZHjYzUTAPMMn","outputId":"7160e214-a0f7-40ba-ec89-5db60e130400","execution":{"iopub.status.busy":"2022-09-09T08:13:52.908822Z","iopub.execute_input":"2022-09-09T08:13:52.909279Z","iopub.status.idle":"2022-09-09T08:13:52.926203Z","shell.execute_reply.started":"2022-09-09T08:13:52.909245Z","shell.execute_reply":"2022-09-09T08:13:52.925121Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data for federated learning","metadata":{"id":"3kR9AMkCPMMn"}},{"cell_type":"code","source":"def split_and_shuffle_labels(y_data, seed, amount):\n    y_data=pd.DataFrame(y_data,columns=[\"labels\"])\n    y_data[\"i\"]=np.arange(len(y_data))\n    label_dict = dict()\n    for i in range(21):\n        var_name=  output_mapping[i]\n        label_info=y_data[y_data[\"labels\"]==i]\n        np.random.seed(seed)\n        label_info=np.random.permutation(label_info)\n        label_info=label_info[0:amount]\n        label_info=pd.DataFrame(label_info, columns=[\"labels\",\"i\"])\n        label_dict.update({var_name: label_info })\n    return label_dict","metadata":{"id":"g5EMbkaVPMMo","execution":{"iopub.status.busy":"2022-09-09T08:13:52.927869Z","iopub.execute_input":"2022-09-09T08:13:52.928220Z","iopub.status.idle":"2022-09-09T08:13:52.935913Z","shell.execute_reply.started":"2022-09-09T08:13:52.928186Z","shell.execute_reply":"2022-09-09T08:13:52.934804Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"This function splis the data with respect to the labels to 21 dataframes (each dataframe contains one unique label and the indices of the different pîctures annotated with that label).\n\nThey are stored in a dictionnary where the key is the label and the value is the considered dataframe.","metadata":{"id":"djy7CPpQPMMp"}},{"cell_type":"code","source":"def get_iid_subsamples_indices(label_dict, number_of_samples, amount):\n    sample_dict= dict()\n    batch_size=int(math.floor(amount/number_of_samples))\n    print(batch_size)\n    for i in range(number_of_samples):\n        sample_name=\"sample\"+str(i)\n        dumb=pd.DataFrame()\n        for j in range(21):\n            label_name=output_mapping[j]\n            a=label_dict[label_name][i*batch_size:(i+1)*batch_size]\n            dumb=pd.concat([dumb,a], axis=0)\n        dumb.reset_index(drop=True, inplace=True)    \n        sample_dict.update({sample_name: dumb}) \n    return sample_dict","metadata":{"id":"gmG1SsDQPMMr","execution":{"iopub.status.busy":"2022-09-09T08:13:52.937971Z","iopub.execute_input":"2022-09-09T08:13:52.938487Z","iopub.status.idle":"2022-09-09T08:13:52.950389Z","shell.execute_reply.started":"2022-09-09T08:13:52.938449Z","shell.execute_reply":"2022-09-09T08:13:52.949457Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"This function returns a dictionnary of 8 keys (8 devices), each key is given by \"sample+i\" and contains the same number of samples equally distributed to all the devices, except for the last device which takes the remaining pictures.","metadata":{"id":"44t5kaLLPMMs"}},{"cell_type":"code","source":"label_dict_train=split_and_shuffle_labels(y_data=y_train, seed=1, amount=90)\nsample_dict_train=get_iid_subsamples_indices(label_dict=label_dict_train, number_of_samples=8, amount=90)\n\n\nlabel_dict_test=split_and_shuffle_labels(y_data=y_test, seed=1, amount=8)\nsample_dict_test=get_iid_subsamples_indices(label_dict=label_dict_test, number_of_samples=8, amount=8)","metadata":{"id":"IiwRw29BPMMs","execution":{"iopub.status.busy":"2022-09-09T08:13:52.952180Z","iopub.execute_input":"2022-09-09T08:13:52.952735Z","iopub.status.idle":"2022-09-09T08:13:53.105878Z","shell.execute_reply.started":"2022-09-09T08:13:52.952692Z","shell.execute_reply":"2022-09-09T08:13:53.104818Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"sample_dict_train['sample0']['labels'] ","metadata":{"execution":{"iopub.status.busy":"2022-09-09T08:13:53.107371Z","iopub.execute_input":"2022-09-09T08:13:53.107959Z","iopub.status.idle":"2022-09-09T08:13:53.116984Z","shell.execute_reply.started":"2022-09-09T08:13:53.107918Z","shell.execute_reply":"2022-09-09T08:13:53.115967Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Each sample contains roughly around 231 images and the last sample contains the rest of the images","metadata":{}},{"cell_type":"code","source":"def create_iid_subsamples(sample_dict, x_data, y_data, x_name, y_name):\n    x_data_dict= dict()\n    y_data_dict= dict()\n    \n    for i in range(len(sample_dict)): \n        xname= x_name+str(i)\n        yname= y_name+str(i)\n        sample_name=\"sample\"+str(i)\n        \n        indices=np.sort(np.array(sample_dict[sample_name][\"i\"]))\n        \n        x_info= x_data[indices,:]\n        x_data_dict.update({xname : x_info})\n        \n        y_info= y_data[indices]\n        y_data_dict.update({yname : y_info})\n        \n    return x_data_dict, y_data_dict","metadata":{"id":"JhnraypVPMMt","execution":{"iopub.status.busy":"2022-09-09T08:13:53.118456Z","iopub.execute_input":"2022-09-09T08:13:53.119462Z","iopub.status.idle":"2022-09-09T08:13:53.127880Z","shell.execute_reply.started":"2022-09-09T08:13:53.119394Z","shell.execute_reply":"2022-09-09T08:13:53.126870Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"This functions extracts x_train, y_train, x_test and y_test dictionnaries from the latest obtained dictionnary. Each element from the dictionnary is specific to a device. ","metadata":{"id":"eKwsPgMBPMMt"}},{"cell_type":"code","source":"x_train_dict, y_train_dict = create_iid_subsamples(sample_dict=sample_dict_train, x_data=x_train, y_data=y_train, x_name=\"x_train\", y_name=\"y_train\")\nx_test_dict, y_test_dict = create_iid_subsamples(sample_dict=sample_dict_test, x_data=x_test, y_data=y_test, x_name=\"x_test\", y_name=\"y_test\")","metadata":{"id":"HNeONlmHPMMu","execution":{"iopub.status.busy":"2022-09-09T08:13:53.131067Z","iopub.execute_input":"2022-09-09T08:13:53.132660Z","iopub.status.idle":"2022-09-09T08:13:54.799131Z","shell.execute_reply.started":"2022-09-09T08:13:53.132623Z","shell.execute_reply":"2022-09-09T08:13:54.797927Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# def compare_local_and_merged_model_performance(number_of_samples):\n#     accuracy_table=pd.DataFrame(data=np.zeros((number_of_samples,3)), columns=[\"sample\", \"local_ind_model\", \"merged_main_model\"])\n#     for i in range (number_of_samples):\n    \n#         test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n#         test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n    \n#         model=model_dict[name_of_models[i]]\n#         criterion=criterion_dict[name_of_criterions[i]]\n#         optimizer=optimizer_dict[name_of_optimizers[i]]\n    \n#         individual_loss, individual_accuracy = validation(model, test_dl, criterion)\n#         main_loss, main_accuracy =validation(main_model, test_dl, main_criterion )\n    \n#         accuracy_table.loc[i, \"sample\"]=\"sample \"+str(i)\n#         accuracy_table.loc[i, \"local_ind_model\"] = individual_accuracy\n#         accuracy_table.loc[i, \"merged_main_model\"] = main_accuracy\n\n#     return accuracy_table","metadata":{"id":"iUCwc9FMPMMw","execution":{"iopub.status.busy":"2022-09-09T08:13:54.802872Z","iopub.execute_input":"2022-09-09T08:13:54.803586Z","iopub.status.idle":"2022-09-09T08:13:54.809235Z","shell.execute_reply.started":"2022-09-09T08:13:54.803527Z","shell.execute_reply":"2022-09-09T08:13:54.808112Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"S15wUohePMMw"}},{"cell_type":"code","source":"class cnn(nn.Module):\n    def __init__(self):\n        super(cnn, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=5, padding=1),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        \n        self.fc1 = nn.Linear(in_features=16*62*62, out_features=300)\n        self.fc2 = nn.Linear(in_features=300, out_features=120)\n        self.fc3 = nn.Linear(in_features=120, out_features=21)\n        \n    def forward(self, x):\n        out = self.layer1(x.float())\n        print(out.shape)\n        out = self.layer2(out)\n        print(out.shape)\n        out = out.view(out.size(0), -1)\n        print(out.shape)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        x = self.fc3(out)\n        \n        return x\n\n# class cnn(nn.Module):\n#     def __init__(self):\n#         super(cnn, self).__init__()\n#         self.conv1 = nn.Conv2d(3, 12, 5, 1,1)\n#         self.conv2 = nn.Conv2d(12,20,5,1,1)\n#         self.fc1 = nn.Linear(20*62*62, 120)\n#         self.fc2 = nn.Linear(120, 84)\n#         self.fc3 = nn.Linear(84, 21)\n\n#     def forward(self, x):\n#         x = F.relu(self.conv1(x))\n#         x = F.max_pool2d(x, 2, 2)\n#         x = F.relu(self.conv2(x))\n#         x = F.max_pool2d(x, 2, 2)\n#         x = x.view(-1, 20*62*62)\n#         x = F.relu(self.fc1(x))\n#         x = F.relu(self.fc2(x))\n#         x = self.fc3(x)\n#         return F.log_softmax(x, dim=1)","metadata":{"id":"cH6-2QIuPMMw","execution":{"iopub.status.busy":"2022-09-09T08:13:54.810748Z","iopub.execute_input":"2022-09-09T08:13:54.811196Z","iopub.status.idle":"2022-09-09T08:13:54.824923Z","shell.execute_reply.started":"2022-09-09T08:13:54.811161Z","shell.execute_reply":"2022-09-09T08:13:54.823968Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def create_model_optimizer_criterion_dict(number_of_samples):\n    model_dict = dict()\n    optimizer_dict= dict()\n    criterion_dict = dict()\n    \n    for i in range(number_of_samples):\n        model_name=\"model\"+str(i)\n        print(model_name)\n        model_info=cnn()\n        model_dict.update({model_name : model_info })\n        \n        optimizer_name=\"optimizer\"+str(i)\n        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n        optimizer_dict.update({optimizer_name : optimizer_info })\n        \n        criterion_name = \"criterion\"+str(i)\n        criterion_info = nn.CrossEntropyLoss()\n        criterion_dict.update({criterion_name : criterion_info})\n        \n    return model_dict, optimizer_dict, criterion_dict ","metadata":{"id":"b9lcrV8XPMMx","execution":{"iopub.status.busy":"2022-09-09T08:13:54.826838Z","iopub.execute_input":"2022-09-09T08:13:54.827328Z","iopub.status.idle":"2022-09-09T08:13:54.840697Z","shell.execute_reply.started":"2022-09-09T08:13:54.827291Z","shell.execute_reply":"2022-09-09T08:13:54.839345Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Similarly to the datasets dictionnaries, we create a local model with respect to each device and we assign each model to its respective key. ","metadata":{"id":"8pfJworuPMMx"}},{"cell_type":"code","source":"# parameters for train\nnumber_of_samples=8\nlearning_rate = 0.03\nnumEpoch = 5\nbatch_size = 32\nmomentum = 0.9","metadata":{"id":"CnF2JHAKPMMy","execution":{"iopub.status.busy":"2022-09-09T08:13:54.844455Z","iopub.execute_input":"2022-09-09T08:13:54.844834Z","iopub.status.idle":"2022-09-09T08:13:54.855253Z","shell.execute_reply.started":"2022-09-09T08:13:54.844801Z","shell.execute_reply":"2022-09-09T08:13:54.854201Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def get_averaged_weights(model_dict, number_of_samples):\n   \n    fc1_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc1.weight.shape)\n    fc1_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc1.bias.shape)\n    \n    \n    fc2_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc2.weight.shape)\n    fc2_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc2.bias.shape)\n    \n    fc3_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc3.weight.shape)\n    fc3_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc3.bias.shape)\n    \n    conv2d1_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].layer1[0].weight.shape)\n    conv2d1_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].layer1[0].bias.shape)\n\n    conv2d2_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].layer2[0].weight.shape)\n    conv2d2_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].layer2[0].bias.shape)\n    \n    with torch.no_grad():\n    \n    \n        for i in range(number_of_samples):\n            fc1_mean_weight += model_dict[name_of_models[i]].fc1.weight.data.clone()\n            fc1_mean_bias += model_dict[name_of_models[i]].fc1.bias.data.clone()\n        \n            fc2_mean_weight += model_dict[name_of_models[i]].fc2.weight.data.clone()\n            fc2_mean_bias += model_dict[name_of_models[i]].fc2.bias.data.clone()\n        \n            fc3_mean_weight += model_dict[name_of_models[i]].fc3.weight.data.clone()\n            fc3_mean_bias += model_dict[name_of_models[i]].fc3.bias.data.clone()\n            \n            conv2d1_mean_weight+=model_dict[name_of_models[0]].layer1[0].weight.data.clone()\n            conv2d1_mean_bias+=model_dict[name_of_models[0]].layer1[0].bias.data.clone()\n\n            conv2d2_mean_weight+=model_dict[name_of_models[0]].layer2[0].weight.data.clone()\n            conv2d2_mean_bias+=model_dict[name_of_models[0]].layer2[0].bias.data.clone()\n        \n        fc1_mean_weight =fc1_mean_weight/number_of_samples\n        fc1_mean_bias = fc1_mean_bias/ number_of_samples\n    \n        fc2_mean_weight =fc2_mean_weight/number_of_samples\n        fc2_mean_bias = fc2_mean_bias/ number_of_samples\n    \n        fc3_mean_weight =fc3_mean_weight/number_of_samples\n        fc3_mean_bias = fc3_mean_bias/ number_of_samples\n        \n        conv2d1_mean_weight =conv2d1_mean_weight/number_of_samples\n        conv2d1_mean_bias = conv2d1_mean_bias/ number_of_samples\n        \n        conv2d2_mean_weight =conv2d2_mean_weight/number_of_samples\n        conv2d2_mean_bias = conv2d2_mean_bias/ number_of_samples\n   \n    return fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias, conv2d1_mean_weight, conv2d1_mean_bias, conv2d2_mean_weight, conv2d2_mean_bias\n","metadata":{"id":"BD0HXL69PMMy","execution":{"iopub.status.busy":"2022-09-09T08:13:54.856493Z","iopub.execute_input":"2022-09-09T08:13:54.858943Z","iopub.status.idle":"2022-09-09T08:13:54.874123Z","shell.execute_reply.started":"2022-09-09T08:13:54.858911Z","shell.execute_reply":"2022-09-09T08:13:54.873155Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"This function takes the average of weights & biases in individual nodes.\nWe started by initializing an instant for both weights and biases of each layer, then fill it with the updated weights and biases of each node (i.e sample).\nFinally, we calculate the average  weight and biases from all the collected parameters of all samples. \n","metadata":{"id":"WoWFw0uHPMMz"}},{"cell_type":"code","source":"def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples):\n    fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias, conv2d1_mean_weight, conv2d1_mean_bias, conv2d2_mean_weight, conv2d2_mean_bias = get_averaged_weights(model_dict, number_of_samples=number_of_samples)\n    \n    with torch.no_grad():\n        main_model.fc1.weight.data = fc1_mean_weight.data.clone()\n        main_model.fc2.weight.data = fc2_mean_weight.data.clone()\n        main_model.fc3.weight.data = fc3_mean_weight.data.clone()\n        main_model.layer1[0].weight.data = conv2d1_mean_weight.data.clone()\n        main_model.layer2[0].weight.data = conv2d2_mean_weight.data.clone()\n\n        main_model.fc1.bias.data = fc1_mean_bias.data.clone()\n        main_model.fc2.bias.data = fc2_mean_bias.data.clone()\n        main_model.fc3.bias.data = fc3_mean_bias.data.clone() \n        main_model.layer1[0].bias.data = conv2d1_mean_bias.data.clone()\n        main_model.layer2[0].bias.data = conv2d2_mean_bias.data.clone()\n\n    return main_model","metadata":{"id":"JJIBomIMPMMz","execution":{"iopub.status.busy":"2022-09-09T08:13:54.875544Z","iopub.execute_input":"2022-09-09T08:13:54.876429Z","iopub.status.idle":"2022-09-09T08:13:54.889611Z","shell.execute_reply.started":"2022-09-09T08:13:54.876374Z","shell.execute_reply":"2022-09-09T08:13:54.888637Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"This function sends the averaged weights of individual local devices to the main model and sets them as the new weights of the main model.","metadata":{"id":"k79hPwnLPMM0"}},{"cell_type":"code","source":"def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples):\n    with torch.no_grad():\n        for i in range(number_of_samples):\n\n            model_dict[name_of_models[i]].fc1.weight.data =main_model.fc1.weight.data.clone()\n            model_dict[name_of_models[i]].fc2.weight.data =main_model.fc2.weight.data.clone()\n            model_dict[name_of_models[i]].fc3.weight.data =main_model.fc3.weight.data.clone()\n            model_dict[name_of_models[i]].layer1[0].weight.data =main_model.layer1[0].weight.data.clone()\n            model_dict[name_of_models[i]].layer2[0].weight.data =main_model.layer2[0].weight.data.clone()\n            \n            model_dict[name_of_models[i]].fc1.bias.data =main_model.fc1.bias.data.clone()\n            model_dict[name_of_models[i]].fc2.bias.data =main_model.fc2.bias.data.clone()\n            model_dict[name_of_models[i]].fc3.bias.data =main_model.fc3.bias.data.clone() \n            model_dict[name_of_models[i]].layer1[0].bias.data =main_model.layer1[0].bias.data.clone()\n            model_dict[name_of_models[i]].layer2[0].bias.data =main_model.layer2[0].bias.data.clone()\n    \n    return model_dict","metadata":{"id":"4nnQP_5OPMM0","execution":{"iopub.status.busy":"2022-09-09T08:13:54.891034Z","iopub.execute_input":"2022-09-09T08:13:54.891520Z","iopub.status.idle":"2022-09-09T08:13:54.904582Z","shell.execute_reply.started":"2022-09-09T08:13:54.891484Z","shell.execute_reply":"2022-09-09T08:13:54.903364Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"This function sends the parameters of the main model to the nodes. It returns an updated version of  model_dict.\n","metadata":{"id":"W9ksiWAbPMM0"}},{"cell_type":"code","source":"def train(model, train_loader, criterion, optimizer):\n    model.train()\n    train_loss = 0.0\n    correct = 0\n\n    for data, target in train_loader:\n        output = model(data).cuda()\n        loss = criterion(output, target.cuda().type( torch.int64))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        prediction = output.argmax(dim=1, keepdim=True).cuda()\n        correct += prediction.cpu().eq(target.cpu().view_as(prediction.cpu())).sum().item()\n        \n\n    return train_loss / len(train_loader), correct/len(train_loader.dataset)","metadata":{"id":"AoGratZ5PMM2","execution":{"iopub.status.busy":"2022-09-09T08:13:54.911517Z","iopub.execute_input":"2022-09-09T08:13:54.911900Z","iopub.status.idle":"2022-09-09T08:13:54.921075Z","shell.execute_reply.started":"2022-09-09T08:13:54.911869Z","shell.execute_reply":"2022-09-09T08:13:54.919670Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"This function is used to train a model on its data (train_loader), with a specified criterion and optimizer. ","metadata":{"id":"BcgQTHwnPMM3"}},{"cell_type":"code","source":"def validation(model, test_loader, criterion):\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            output = model(data).cuda()\n            test_loss += criterion(output, target.cuda().type( torch.int64) ).item()\n            prediction = output.argmax(dim=1, keepdim=True).cuda()\n            correct += prediction.cpu().eq(target.cpu().view_as(prediction.cpu())).sum().item()\n\n    test_loss /= len(test_loader)\n    correct /= len(test_loader.dataset)\n\n    return (test_loss, correct)\n","metadata":{"id":"j3SK0Xq_PMM3","execution":{"iopub.status.busy":"2022-09-09T08:13:54.922838Z","iopub.execute_input":"2022-09-09T08:13:54.923348Z","iopub.status.idle":"2022-09-09T08:13:54.931773Z","shell.execute_reply.started":"2022-09-09T08:13:54.923301Z","shell.execute_reply":"2022-09-09T08:13:54.930681Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"This function evaluates the model on the test data","metadata":{"id":"cw7mDY2dPMM3"}},{"cell_type":"code","source":"def start_train_end_node_process(number_of_samples):\n    for i in range (number_of_samples): \n\n        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n\n        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n        test_dl = DataLoader(test_ds, batch_size=batch_size*2 )\n        \n        \n    \n        model=model_dict[name_of_models[i]]\n        criterion=criterion_dict[name_of_criterions[i]]\n        optimizer=optimizer_dict[name_of_optimizers[i]]\n    \n        print(\"Subset\" ,i)\n        for epoch in range(numEpoch):        \n            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n            val_loss, val_accuracy = validation(model, test_dl, criterion)\n    \n            print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | validation accuracy: {:7.5f}\".format(val_accuracy))","metadata":{"id":"oBN4AvL9PMM4","execution":{"iopub.status.busy":"2022-09-09T08:13:54.933501Z","iopub.execute_input":"2022-09-09T08:13:54.933925Z","iopub.status.idle":"2022-09-09T08:13:54.946229Z","shell.execute_reply.started":"2022-09-09T08:13:54.933890Z","shell.execute_reply":"2022-09-09T08:13:54.945184Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"This function does the train of a specific node (a local device) for 5 epochs on its own mini-dataset.","metadata":{"id":"oPDZs-B2PMM4"}},{"cell_type":"code","source":"def start_train_end_node_process_without_print(number_of_samples):\n    for i in range (number_of_samples): \n\n        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n\n        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n        test_dl = DataLoader(test_ds, batch_size=batch_size)\n        \n        \n    \n        model=model_dict[name_of_models[i]]\n        criterion=criterion_dict[name_of_criterions[i]]\n        optimizer=optimizer_dict[name_of_optimizers[i]]\n    \n        for epoch in range(numEpoch):        \n            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n            val_loss, val_accuracy = validation(model, test_dl, criterion)\n","metadata":{"id":"1WpUHNn-PMM4","execution":{"iopub.status.busy":"2022-09-09T08:13:54.949615Z","iopub.execute_input":"2022-09-09T08:13:54.949905Z","iopub.status.idle":"2022-09-09T08:13:54.959099Z","shell.execute_reply.started":"2022-09-09T08:13:54.949879Z","shell.execute_reply":"2022-09-09T08:13:54.958072Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"This is the same previous function except that we don't print the process in each node.","metadata":{"id":"KuiYQlSaPMM8"}},{"cell_type":"code","source":"main_model = cnn()\nmain_optimizer = torch.optim.SGD(main_model.parameters(), lr=0.01, momentum=0.9)\nmain_criterion = nn.CrossEntropyLoss()","metadata":{"id":"PLlZPUJuPMM9","execution":{"iopub.status.busy":"2022-09-09T08:13:54.960567Z","iopub.execute_input":"2022-09-09T08:13:54.961688Z","iopub.status.idle":"2022-09-09T08:13:55.156284Z","shell.execute_reply.started":"2022-09-09T08:13:54.961648Z","shell.execute_reply":"2022-09-09T08:13:55.155287Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"The created model is a Convolutional network. We create an instance from the defined cnn class along with the SGD optimizer and CrossEntropyLoss to track the error.","metadata":{"id":"iDRHERKUPMM_"}},{"cell_type":"code","source":"model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_samples)\n#fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias, conv2d1_mean_weight, conv2d1_mean_bias, conv2d2_mean_weight, conv2d2_mean_bias = get_averaged_weights(model_dict, number_of_samples)","metadata":{"id":"8qWFABYnPMNA","execution":{"iopub.status.busy":"2022-09-09T08:13:55.157969Z","iopub.execute_input":"2022-09-09T08:13:55.158346Z","iopub.status.idle":"2022-09-09T08:13:57.046251Z","shell.execute_reply.started":"2022-09-09T08:13:55.158307Z","shell.execute_reply":"2022-09-09T08:13:57.044666Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#List of the names of data partitions\nname_of_x_train_sets=list(x_train_dict.keys())\nname_of_y_train_sets=list(y_train_dict.keys())\nname_of_x_test_sets=list(x_test_dict.keys())\nname_of_y_test_sets=list(y_test_dict.keys())\n\n#List of the names of models, optimizers and criterions\nname_of_models=list(model_dict.keys())\nname_of_optimizers=list(optimizer_dict.keys())\nname_of_criterions=list(criterion_dict.keys())\n","metadata":{"id":"864oKqkhPMNA","execution":{"iopub.status.busy":"2022-09-09T08:13:57.048508Z","iopub.execute_input":"2022-09-09T08:13:57.049429Z","iopub.status.idle":"2022-09-09T08:13:57.086395Z","shell.execute_reply.started":"2022-09-09T08:13:57.049364Z","shell.execute_reply":"2022-09-09T08:13:57.061137Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print(main_model.fc2.weight[0:1,0:5])\nprint(model_dict[\"model1\"].fc2.weight[0:1,0:5])","metadata":{"id":"YC8IiqUlPMNB","outputId":"9815aa6e-45eb-4a24-b32c-6baf44d186fb","execution":{"iopub.status.busy":"2022-09-09T08:13:57.094327Z","iopub.execute_input":"2022-09-09T08:13:57.099625Z","iopub.status.idle":"2022-09-09T08:13:57.158735Z","shell.execute_reply.started":"2022-09-09T08:13:57.099568Z","shell.execute_reply":"2022-09-09T08:13:57.155473Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples)","metadata":{"id":"04e7LWyoPMNB","execution":{"iopub.status.busy":"2022-09-09T08:13:57.165649Z","iopub.execute_input":"2022-09-09T08:13:57.168585Z","iopub.status.idle":"2022-09-09T08:13:57.814690Z","shell.execute_reply.started":"2022-09-09T08:13:57.168495Z","shell.execute_reply":"2022-09-09T08:13:57.813470Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"print(main_model.fc2.weight[0:1,0:5])\nprint(model_dict[\"model1\"].fc2.weight[0:1,0:5])","metadata":{"id":"FFxFLfX-PMNC","outputId":"1d57e343-b060-45fe-96a6-29b436e3c866","execution":{"iopub.status.busy":"2022-09-09T08:13:57.821090Z","iopub.execute_input":"2022-09-09T08:13:57.823136Z","iopub.status.idle":"2022-09-09T08:13:57.838977Z","shell.execute_reply.started":"2022-09-09T08:13:57.823094Z","shell.execute_reply":"2022-09-09T08:13:57.837557Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"We can see that the updates from the main model have been made successfully.","metadata":{"id":"my4UvwPiQMGA"}},{"cell_type":"code","source":"start_train_end_node_process(number_of_samples)","metadata":{"id":"V5MmYxH3PMNC","execution":{"iopub.status.busy":"2022-09-09T08:13:57.842493Z","iopub.execute_input":"2022-09-09T08:13:57.844149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This training is only done for 1 iteration.\nWe can see that the accuracies are not that great.\n\nIt's normal because we haven't sent the local trained weights to the main model. Once we send the weights to the main model, they're averaged and sent back to the local nodes. Then, this process is repeated until we achieve 10 iterations.","metadata":{"id":"CJeEuZzjPMND"}},{"cell_type":"code","source":"test_ds = TensorDataset(x_test, y_test)\ntest_dl = DataLoader(test_ds, batch_size=batch_size * 2)","metadata":{"id":"bOAYRt5sPMND","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training local models for 1 iteration and after sending the weights and before averaging them by the main model we can see that the main model has very bad accuracy because it was randomly initialiazed. \n\\\nWe must send the weights of the local nodes to the main i order to average them and get more reasonable accuracy.","metadata":{"id":"yQlhmuZLR3-6"}},{"cell_type":"markdown","source":"After training local models for 1 iteration and after sending the weights and after averaging them by the main model we can see that the main model still has medium accuracy. \n\\\nWe must train for more iterations in local nodes and repeat the same process.","metadata":{"id":"6AeZbS3oQkXD"}},{"cell_type":"code","source":"for i in range(10):\n    #send weights to local nodes\n    model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples)\n    #train local nodes\n    start_train_end_node_process_without_print(number_of_samples)\n    \n    main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples) \n    test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n    print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))","metadata":{"id":"slaHTQN1PMNE","outputId":"abaa4747-2135-42df-a9fd-9806c40ea023","execution":{"iopub.status.busy":"2022-09-09T08:54:27.986514Z","iopub.execute_input":"2022-09-09T08:54:27.987672Z","iopub.status.idle":"2022-09-09T11:39:55.831970Z","shell.execute_reply.started":"2022-09-09T08:54:27.987631Z","shell.execute_reply":"2022-09-09T11:39:55.830811Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"The same iterations are repeated 10 times and the accuracy improves gradually as the main model learns more from the local nodes each iteration.\n\\\nIt is not a perfect result and may be less performant than the normal training on ine unique device; but the results are quite intresting.","metadata":{"id":"4ECld_lwSt54"}},{"cell_type":"code","source":"","metadata":{"id":"rJ7WLiHkq5Eh"},"execution_count":null,"outputs":[]}]}